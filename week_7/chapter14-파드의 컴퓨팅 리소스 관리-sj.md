<aside>
💡

- 컨테이너의 CPU, 메모리, 그 밖의 컴퓨팅 리소스 요청
- CPU와 메모리에 대한 엄격한 제한 설정
- 파드에 대한 **서비스 품질 보장** 이해
- 네임스페이스에서 파드의 기본, 최소, 최대 리소스 설정
- 네임스페이스에서 사용 가능한 리소스의 총량 제한
</aside>

## 1. 파드 컨테이너의 리소스 요청

- 파드를 생성할 때 컨테이너가
    - 필요로 하는 CPU와 메모리 양 (request)
    - 사용할 수 있는 CPU와 메모리 양 (limit)
    
    을 지정할 수 있다.
    

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-limited-pod
  namespace: default
  labels:
    app: resource-example
spec:
  containers:
  - name: app-container
    image: nginx:latest
    resources:
      requests:
        memory: "128Mi"    # 요청 메모리: 128 MiB
        cpu: "100m"        # 요청 CPU: 100 milliCPU (0.1 CPU 코어)
      limits:
        memory: "256Mi"    # 최대 사용 메모리: 256 MiB
        cpu: "500m"        # 최대 사용 CPU: 500 milliCPU (0.5 CPU 코어)
    ports:
    - containerPort: 80
```

- 리소스 요청이 스케줄링에 미치는 영향
    - 리소스 요청을 지정하면 파드에 필요한 리소스의 최소량을 지정할 수 있다.
    - 스케줄러가 노드에 파드를 스케줄링할 때 이 정보를 사용한다.
    - 이때 실제 노드에 배포된 파드들이 실제로 얼마의 리소스를 사용하는가와 상관 없이 **리소스 요청량의 전체 합**만을 고려한다.
    
    <img width="664" alt="image" src="https://github.com/user-attachments/assets/cd33cde6-ec33-43ce-8bfd-8314299ca83d" />

- CPU 요청이 CPU 시간 공유에 미치는 영향
    - CPU 제한 없이 요청만 있을 경우, CPU 시간을 파드 간에 분배하는 방식도 요청에 의해 결정된다.
    - 예를 들어 A 파드가 200밀리코어를 요청하고 B 파드가 1000밀리코어를 요청하면, 미사용된 CPU는 두 파드 사이에 1:5 비율로 분배된다. 두 파드가 가능한 한 많은 CPU를 소비하는 경우,  A파드는 1/6의 CPU 시간을 얻고 다른 파드는 나머지 5/6을 얻는다.
    - 그러나 한 컨테이너가 CPU를 최대로 사용하려는 순간 나머지 파드가 유휴 상태에 있다면 첫 번째 컨테이너가 전체 CPU 시간을 사용할 수 있다.

## 2. 컨테이너에 사용 가능한 리소스 제한

- CPU는 압축 가능한 리소스인 반면, 메모리는 압축이 불가능하다.
    - 컨테이너가 CPU 한계를 초과하려고 하면, 쿠버네티스는 CPU 시간을 제한함 
    → 응용 프로그램이 더 느리게 실행되지만, 강제로 종료되지는 않음
    - 컨테이너가 메모리 한계를 초과하면, 쿠버네티스는 해당 컨테이너를 종료(OOMKilled)시킴
        - 예: 메모리 한계가 256MB인 컨테이너가 300MB를 사용하려고 하면, 쿠버네티스는 해당 컨테이너를 종료하고 다시 시작
- 리소스 제한 오버커밋
    - 노드에 있는 모든 파드의 리소스 제한 합계는 노드 용량의 100%를 초과할 수 있다.
    - 즉, 리소스 제한은 오버커밋될 수 있다.
    - 노드 리소스의 100%가 다 소진되면 특정 컨테이너는 제거되어야 한다.
- 메모리 리소스 제한 초과와 CrashLoopBackOff
    - 메모리 제한 초과와 종료가 지속되면 쿠버네티스는 재시작 사이의 지연 시간을 증가시키면서 재시작시키는데, 이런 경우 CrashLoopBackOff 상태가 표시된다.
    - 첫 번째 크래시: Kubelet은 컨테이너를 즉시 다시 시작
    두번째 크래시: 다시 시작하기 전에 10초를 기다림
    n번째 크래시: 20초, 40초, 80초, 160초로 지수로 증가하고 마지막으로 300초로 제한
    300초에 도달하면 kubelet은 크래시를 멈추거나 삭제될 때까지 5분마다 컨테이너를 계속 재시작함
- 컨테이너의 애플리케이션이 제한을 바라보는 방법
    - 컨테이너에서 top 명령을 쳐보면 컨테이너가 실행 중인 **전체 노드의 CPU / 메모리 양을 표시**하고 있다. (컨테이너는 제한을 인식하지 못함)
    - 자바 애플리케이션을 실행할 때 특히 -Xms 옵션으로 JVM의 최대 힙 크기를 지정하지 않는 경우 문제가 발생한다. → JVM은 컨테이너에 사용 가능한 메모리 대신 호스트의 총 메모리를 기준으로 최대 힙 크기를 설정함. → OOM이 날 수 있다.
    - -Xms 옵션은 힙 크기를 제한하지만 JVM의 오프 힙 메모리에는 영향을 미치지 않는다. 새 버전의 자바는 컨테이너 제한 설정을 고려한다고 함

## 3. 파드 QoS 클래스 이해

- 리소스 제한은 오버 커밋될 수 있다.
- 두 개의 파드가 있다고 가정해보자. 파드 A는 노드 메모리의 90%를 사용하고 있는 상황에서 파드 B가 갑자기 그 시점까지 사용하던 메모리보다 많은 메모리를 요구해 노드가 필요한 양의 메모리를 제공할 수 없다면? 어떤 컨테이너를 종료해야 하는가?
- 이런 경우 어떤 파드가 우선순위를 가지는지 지정해야 하며, 쿠버네티스는 파드를 3가지 서비스 품질 (Qos, Quality of Service) 클래스로 분류한다.
    - BestEffort (최하위 우선순위)
    - Burstable
    - Guaranteed (최상위 우선순위)
- QoS 클래스는 파드 매니페스트로 정의되지 않는다. **파드 컨테이너의 리소스 요청과 제한의 조합에서 파생된다.**
    - BestEffort : 아무런 리소스 요청과 제한이 없는 (파드와 컨테이너 중 하나도 없는) 파드에 할당됨. 즉 이런 파드에 실행 중인 컨테이너는 리소스 보장을 받지 못한다. 다른 파드를 위해 메모리가 해제되어야 할 때 가장 먼저 종료됨. 그러나 메모리가 충분하다면 원하는 만큼 메모리를 사용할 수 있음
    - Guaranteed : 아래와 같은 조건을 만족해야 하며, 이런 파드의 컨테이너는 요청된 리소스의 양을 얻지만 추가 리소스를 사용할 수 없다 (리소스 제한 = 요청이므로)
        - CPU와 메모리에 리소스 요청과 제한이 모두 설정돼야 함
        - 각 컨테이너에 설정돼야 함
        - 리소스 요청과 제한이 동일해야 함 (각 컨테이너의 각 리소스에 관한 리소스 제한이 요청과 일치해야 함
    - Burstable QoS
        - BestEffort와 Guaranteed 외 모든 파드가 여기에 해당된다.
        - 요청한 양 만큼의 리소스를 얻지만 필요하면 추가 리소스 (리소스 제한까지)를 사용할 수 있다.
- 메모리가 부족할 때 어떤 프로세스가 종료되는가?
    - BestEffort 클래스가 가장 먼저 종료 → Burstable → Guaranteed (시스템 프로세스가 메모리를 필요로하는 경우에만 종료)
    - 동일 QoS 클래스를 갖는 컨테이너라면?
        - 실행 중인 각 프로세스가 갖는 OOM 점수를 비교하여 메모리 해제가 필요할때 가장 높은 점수의 프로세스를 종료함
        - 요청된 메모리의 사용률이 더 높은 파드를 먼저 종료한다.(아래 그림에서 B 파드가 C 파드보다 먼저 종료된다.)
    
    <img width="664" alt="image" src="https://github.com/user-attachments/assets/6c3fca45-01ea-4b8d-8e97-333cba8ea998" />


## 4. 네임스페이스별 파드에 대한 기본 요청과 제한 설정

- LimitRange
    - 특정 네임스페이스 내의 리소스 사용량을 제한하는 데 사용되는 객체 (개별 컨테이너 또는 파드 수준에서 리소스 제한을 관리)
    - 네임스페이스 내에서 생성되는 모든 파드와 컨테이너에 대한 기본 리소스 요청 및 제한을 설정하거나 강제한다.
    - LimitRanger 어드미션 컨트롤 플러그인에서 사용됨
    - 파드 매니페스트가 API 서버에 게시되면 LimitRanger 플러그인이 파드 스펙을 검증
    
    ```yaml
    apiVersion: v1
    kind: LimitRange
    metadata:
      name: comprehensive-limit-range
    spec:
      limits:
      # Container 레벨 제한
      - type: Container
        default:           # 컨테이너의 기본 리소스 한계
          cpu: "500m"
          memory: "256Mi"
        defaultRequest:    # 컨테이너의 기본 리소스 요청량
          cpu: "100m"
          memory: "128Mi"
        max:               # 컨테이너가 설정할 수 있는 최대값
          cpu: "2"
          memory: "1Gi"
        min:               # 컨테이너가 설정해야 하는 최소값
          cpu: "50m"
          memory: "64Mi"
        maxLimitRequestRatio:  # 제한과 요청 사이의 최대 비율
          cpu: 4
          memory: 2
          
      # Pod 레벨 제한
      - type: Pod
        max:               # 파드 전체(모든 컨테이너 합계)에 대한 최대값
          cpu: "4"
          memory: "2Gi"
        min:               # 파드 전체에 대한 최소값
          cpu: "200m"
          memory: "256Mi"
          
      # PVC가 요청할 수 있는 스토리지의 최소 및 최대량을 설정할 수 있다.
      - type: PersistentVolumeClaim
        max:               # PVC가 요청할 수 있는 최대 스토리지
          storage: "50Gi"
        min:               # PVC가 요청해야 하는 최소 스토리지
          storage: "1Gi"
    ```
    

## 5. 네임스페이스의 사용 가능한 총 리소스 제한하기

- 리소스 쿼터(**ResourceQuota**) 오브젝트
    - 네임스페이스 전체의 총 리소스 사용량을 제한
- 리소스쿼터 어드미션 컨트롤 플러그인이 생성 중인 파드가 설정된 리소스 쿼터를 초과하는지 확인 (초과하면 생성 거부됨)
- 파드를 생성할 때만 적용됨. (기존 파드에는 영향 없음)

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: team-a
spec:
  hard:
    # 컴퓨팅 리소스 제한
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    
    # 스토리지 리소스 제한
    requests.storage: 500Gi
    persistentvolumeclaims: 10
    
    # 특정 스토리지 클래스에 대한 제한
    ssd.storageclass.storage.k8s.io/requests.storage: 300Gi
    
    # 오브젝트 수 제한
    pods: 20
    services: 10
    configmaps: 20
    secrets: 30
    replicationcontrollers: 10
    deployments.apps: 10
    statefulsets.apps: 5
```

## 6. 파드 리소스 사용량 모니터링

- 리소스 요청과 제한을 적절하게 설정하기 위해서는 예상되는 부하 수준에서 컨테이너의 실제 리소스 사용량을 모니터링 해야한다.
- 실제 리소스 사용량 수집과 검색
    - kubelet 자체에 cAdvisor라는 에이전트가 포함되어 있음 → 노드에서 실행되는 개별 컨테이너와 노드 전체의 리소스 사용 데이터를 수집
    - 힙스터라는 추가 구성 요소를 실행하여 이러한 통계를 중앙에서 수집
    
    <img width="690" alt="image" src="https://github.com/user-attachments/assets/ea34204d-d8a9-4975-9bef-33155bc1510b" />

- 기간별 리소스 사용량 통계 저장 및 분석
    - cAdvisor와 힙스터는 모두 짧은 기간 동안의 리소스 사용량 데이터만 보유한다.
    - 통계 데이터 저장을 위해 InfluxDB를, 시각화와 분석을 위해 그라파나를 사용한다.
        - InfluxDB : 오픈소스 시계열 데이터베이스
- 그래프 분석

<img width="690" alt="image" src="https://github.com/user-attachments/assets/54220534-e5cc-4aa6-abca-1bf97aa09076" />

## 7. 요약

- **리소스 요청** 지정은 쿠버네티스가 클러스터 전체에서 **파드를 스케줄링**하는 데 도움이 된다.
- **리소스 제한**을 지정하면 파드가 **다른 파드의 리소스를 고갈시키지 않는다.**
- 사용되지 않는 CPU 시간은 컨테이너의 CPU 요청에 따라 할당된다.
- 컨테이너는 CPU를 너무 많이 사용하는 경우 절대 종료되지 않지만, 메모리를 너무 많이 사용하려고 하면 종료된다.
- 오버커밋된 시스템에서 **파드의 QoS 클래스**와 실제 메모리 사용량에 근거해 좀 더 중요한 파드의 메모리를 확보하기 위해 컨테이너를 종료한다.
- **LimitRange 오브젝트**를 사용해 개별 파드에 관한 최소, 최대, 기본 리소스 요청과 제한을 정의할 수 있다.
- **리소스쿼터 오브젝트**를 사용해 네임스페이스의 모든 파드에서 사용할 수 있는 리소스의 양을 제한할 수 있다.
- 파드의 자원 요청과 제한을 얼마나 높게 설정할지 알기 위해서 충분한 기간 동안 파드가 자원을 어떻게 사용하는지 모니터링해야 한다.
