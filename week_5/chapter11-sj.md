### ETCD

- 쿠버네티스의 시스템 구성 요소는 오직 API 서버하고만 통신한다. 따라서 API 서버는 etcd와 통신하는 유일한 구성 요소다.
- 둘 이상의 etcd 인스턴스를 실행할 수 있음 → 고가용성과 우수한 성능을 제공
- **저장된 오브젝트의 일관성과 유효성 보장**
    - 다른 모든 구성요소가 etcd에 읽기 및 쓰기 작업을 수행할때 반드시 API 서버를 통하도록 하여 **낙관적 잠금 매커니즘**을 구현한다.
    - 낙관적 잠금 매커니즘이란?
        - 데이터에 잠금을 설정해 그동안 데이터를 읽거나 업데이트하지 못하도록 하는 대신, 데이터에 버전 번호를 포함시킴 → 데이터가 업데이트 될 때마다 버전 번호 증가
        - 두 클라이언트가 동시에 같은 데이터에 대해 업데이트를 시도할 때, 클라이언트가 데이터를 읽은 시간과 업데이트를 시도하는 시간 사이에 버전 번호가 증가했는지 여부를 체크 → 만약 버전 번호가 증가했다면 수정된 내용은 거부되고 클라이언트는 다시 새 데이터를 읽고 다시 업데이트를 시도해야 한다. → 즉, 두 클라이언트 중 먼저 업데이트 시도한 것만 성공한다.
        - 모든 쿠버네티스 리소스에는 클라이언트가 오브젝트를 업데이트할 때 API 서버로 같이 넘겨줘야 하는 metadata.resourceVersion 필드가 있다. 만약 etcd에 저장돼 있는 버전과 일치하지 않을 경우 API 서버는 수정 요청을 거부한다.
- **클러스터링된 etcd의 일관성 보장**
    - 2개 이상의 etcd 인스턴스가 일관성을 유지하는 방법 → RAFT 합의 알고리즘 사용
    - RAFT 합의 알고리즘
        - 모든 노드는 아래의 3가지 중 하나의 상태를 가진다. 일반적으로 하나의 리더와 나머지 팔로워들로 구성되며, 후보자는 오직 리더가 없거나 무응답 상태일 경우에만 일시적으로 존재한다.
            1. **리더(Leader)** : 클러스터를 대표하는 하나의 노드다. 리더는 클라이언트가 클러스터로 보낸 모든 명령의 수신 및 전파, 그리고 응답을 전담한다. 또한 리더는 자신의 상태 메시지(heartbeat)를 주기적으로 모든 팔로워에게 전파한다.
            2. **팔로워(Follower)** : 리더가 존재하는 한 나머지 노드는 이 상태를 유지한다. 리더로부터 전파된 명령을 처리하는 역할만 담당한다.
            3. **후보자(Candidate)** : 리더가 없는 상황에서 새 리더를 정하기 위해 전환된 팔로워의 상태를 의미한다. 리더로부터 일정 시간 이상 상태 메시지(heartbeat)를 받지 못한 팔로워는 후보자로 전환된다.
        - 뗏목 합의 알고리즘(Raft Consensus Algorithm)은 **클러스터 전체에 대한 명령이 오직 리더로부터 팔로워에게 일방향으로 전파**되도록 동작한다.
        - 합의 알고리즘 흐름
            1. 리더는 수신된 명령에 대한 **로그(log)**를 생성하여 로컬에 저장한 뒤 **모든 팔로워에게 복제하여 전달**한다. 각 팔로워는 전달받은 로그에 대한 응답을 다시 리더에게 보낸다.
            2. 리더가 수신한 정상 응답 수가 **클러스터 전체 노드의 과반수**에 이르면, 리더는 **로그를 통해 전파된 명령을 클러스터의 모든 노드가 동일하게 수행**하도록 한 뒤 클라이언트에게 명령 수행 결과를 리턴한다. 한편 리더는 해당 로그를 클러스터 전체 노드가 똑같이 보유할 때까지 로그 재전송을 주기적으로 반복한다.
            3. 시스템 문제나 네트워크 이슈로 제때 명령을 처리하지 못한 팔로워가 있더라도, 그 팔로워는 정상 상태로 복구된 뒤 클러스터와의 연결이 재개되면 리더로부터 그동안의 명령 처리 기록이 포함된 로그들을 다시 전달받아 순차적으로 수행한다. **클러스터 전체의 최신화 및 동기화**는 이렇게 하여 유지된다.
        - etcd 인스턴스 수가 홀수인 이유
            - 리더 선출을 하기 위해서는 과반수 이상의 동의가 필요함.
            - etcd 인스턴스가 2개일 경우
                - 둘중 하나가 장애나면 1개의 인스턴스만 남음 → 따라서 과반수 (2개 중 2개 이상)을 충족할 수 없으므로 리더 선출 불가능 → 클러스터 정지
                - 두 인스턴스가 서로 네트워크 연결이 끊길 경우 → 과반수 만족 못해서 클러스터 전체가 멈춤
            - 인스턴스가 1개일 경우
                - 리더 선출 과정이 필요 없음 → 운영 단순
                - 단일 장애점이 됨 (2개거나 1개거나 하나라도 장애가 나면 클러스터 장애가 나는건 똑같음)
            - 인스턴스가 4개일 경우 vs 3개인 경우 → 내성 차이 없음
                - 4개 중 1개 장애 발생 시 → 남은 3개로 정상 동작
                - 그런데 3개 중 1개 장애 발생 시 → 남은 2개로도 정상 동작
                - 즉, 4개를 두는 것과 3개를 두는 것은 장애 내성이 똑같음. 대신 4개를 사용하면 그만큼 리소스가 낭비됨
                - 4개 중 2개 - 2개로 네트워크가 분리되면? 어느쪽도 과반수를 만족 못해서 전체 클러스터가 멈춤
                - 3개 중 2개 - 1개로 네트워크 분리되면? 2개쪽은 정상 동작함

### API 서버

- RESTful API로 CRUD를 제공하여 클러스터 상태를 조회하고 변경할 수 있도록 함.
- 클라이언트는 kubectl 명령으로 API 서버와 통신한다.
- API 서버가 요청을 받고 처리하는 흐름
    - 인증 플러그인으로 클라이언트 인증
        - API 서버에 구성된 하나 이상의 인증 플러그인을 차례로 호출하여 수행
        - 인증 방법에 따라 사용자를 클라이언트 인증서 혹은 HTTP 헤더에서 가져옴
        - 사용자 이름, 사용자 ID, 속해있는 그룹 정보를 추출
    - 인가 플러그인을 통한 클라이언트 인가
        - 하나 이상의 인가 플러그인을 사용하여 인증된 사용자가 요청한 작업을 수행할 수 있는 권한이 있는지 검사
    - 어드미션 컨트롤 플러그인으로 요청된 리소스 검증과 수정
        - 리소스를 생성, 수정, 삭제하려는 요청인 경우에 해당 요청은 어드미션 컨트롤로 보내짐
        - API 서버는 여러 어드미션 컨트롤 플러그인을 사용하여 리소스 정의에서 누락된 필드를 기본값으로 초기화하거나 재정의할 수 있음.
        - 요청에 없는 관계된 리소스를 수정하거나 요청을 거부할 수 있음
    - 리소스 유효성 확인 및 영구 저장
        - 요청이 모든 어드미션 컨트롤 플러그인을 통과하면, API 서버는 오브젝트의 유효성을 검증하고 etcd에 저장한다. 그리고 클라이언트에 응답을 반환함
- API 서버가 리소스 변경을 클라이언트에 통보하는 방법 → 감시(watch) 매커니즘
    - 클라이언트는 API 서버에 HTTP 연결을 맺고 변경 사항을 감지함. 
    `kubectl get pods —watch`
    - 이 연결을 통해 클라이언트는 감시 대상 오브젝트의 변경을 알 수 있는 스트림을 받음.
    - 오브젝트가 갱신될 때마다 서버는 오브젝트를 감시하고 있는 모든 클라이언트에게 오브젝트의 새로운 버전을 보냄

### 파드 간 네트워킹

- 쿠버네티스 네트워크는 어떤 모습이어야 하는가 → 플랫 네트워크
    - 각 파드가 고유한 IP 주소를 가지고 다른 모든 파드와 NAT 없이 플랫 네트워크로 서로 통신할 수 있다.
    - 이는 쿠버네티스 자체가 아닌, 시스템 관리자 또는 컨테이너 네티워크 인터페이스(CNI) 플러그인에 의해 제공된다.
    - **“파드 A가 네트워크 패킷을 보내기 위해 파드 B에 연결할 때, 파드 B가 보는 출발지 IP는 파드 A의 IP 주소와 동일해야 한다.”**
    - **한편, 파드가 인터넷에 있는 서비스와 통신할 때는 패킷의 출발지 IP를 호스트 워커 노드의 IP로 변경하는 작업이 필요하다.**
- 네트워킹 동작 방식
    <img width="804" alt="image" src="https://github.com/user-attachments/assets/586d7bf9-3a55-4399-91d6-05b60f4e24c7" />

    - 파드의 네트워크 인터페이스는 인프라스트럭처 컨테이너에서 설정한 것
    - 동일한 노드에서 파드 간의 통신 활성화
        - 인프라스트럭처 컨테이너가 시작되기 전에, 컨테이너를 위한 **가상 이더넷 인터페이스 쌍(veth 쌍)**이 생성된다.
            - 이 쌍의 한쪽 인터페이스 : 호스트의 네임스페이스 (노드에서 ifconfig를 실행할 때 볼 수 있는 vethXX 목록)에 남아있고
            - 다른 쪽 인터페이스 : 컨테이너의 네트워크 네임스페이스 안으로 옮겨져 이름이 eth0으로 변경된다.
            - 이 두 개의 가상 인터페이스는 파이프의 양쪽 끝과 같다 (또는 이더넷 케이블로 연결된 네트워크 장치). → 한쪽으로 들어가면 다른 쪽으로 나온다. 반대의 경우도 마찬가지
        - 호스트의 네트워크 네임스페이스에 있는 인터페이스 : 컨테이너 런타임이 사용할 수 있도록 설정된 네트워크 브리지에 연결됨
        - 컨테이너 안의 eth0 인터페이스 : 브리지의 주소 범위 안에서 IP를 할당 받는다.
        - 파드 A의 컨테이너 내부에서 실행되는 애플리케이션 → eth0 인터페이스로 전송 → 호스트 네임스페이스의 다른쪽 veth 인터페이스로 나와 → 브리지로 전달 → 파드 B의 veth 쌍을 통과
        - 노드에 있는 모든 컨테이너는 같은 브리지에 연결 → 서로 통신 가능
        - 그러나 다른 노드에서 실행 중인 컨테이너가 서로 통신하려면 이 노드 사이의 브리지가 어떤 형태로든 연결되어 있어야 함
    
    - 서로 다른 노드에서 파드 간의 통신 활성화
        <img width="804" alt="image" src="https://github.com/user-attachments/assets/29814770-864d-4329-a461-60b08223200e" />

        - 파드 IP 주소는 전체 클러스터 내에서 유일해야 하기 때문에, **노드 사이의 브리지는 겹치지 않는 주소 범위를 사용**해서 다른 노드에 있는 파드가 같은 IP 주소를 얻지 못하도록 해야 한다.
        - 노드 간 통신이 가능하기 위해서는 노드의 **물리 네트워크 인터페이스도 브리지에 연결**해야 한다.
        - 노드 A의 **라우팅 테이블**은 10.1.2.0/24로 향하는 모든 패킷이 노드 B로 전달되도록 설정해야 하고,
        노드 B에서는 10.1.1.0/24로 향하는 패킷이 노드 A로 전달되도록 설정하는 것이 필요하다.
        - 그러나 이 방법은 두 노드가 라우터 없이 같은 네트워크 스위치에 연결된 경우에만 동작한다. (라우터는 패킷이 참조하는 파드의 IP가 프라이빗 대역에 속하기 때문에 패킷을 삭제한다)
        - 노드 사이에 있는 라우터가 패킷을 전달하도록 설정할 수는 있지만 복잡함 → 소프트웨어 정의 네트워크 (SDN)을 사용하는 것이 더 쉽다.
        - SDN을 사용하면 파드에서 전송한 패킷은 캡슐화돼 네트워크로 다른 파드가 실행중인 노드로 전달하고, 디캡슐화 단계를 거쳐 원래 패킷 형태로 대상 파드에 전달한다. → 일반적인 네트워크 트래픽처럼 전달 가능
        - 대표적인 SDN 기반 CNI 플러그인:
            - **Flannel** (VXLAN 기반)
            - **Calico** (BGP 기반)
            - **Cilium** (eBPF 기반)
            - **Weave** (Mesh 네트워크 기반)

참고)

https://seongjin.me/raft-consensus-algorithm/
